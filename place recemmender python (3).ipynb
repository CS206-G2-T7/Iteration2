{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72587fdb-15ce-498a-ac16-a82644337e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6db712a-4478-4992-8d20-a6e55b6ecc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  restaurantId  rating\n",
      "0       1           111     3.0\n",
      "1       1           107     4.5\n",
      "2       1            65     4.6\n",
      "3       1            35     3.6\n",
      "4       1            86     4.0\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "#loading rating dataset\n",
    "ratings = pd.read_csv(\"C:/Users/Garion/Desktop/user preference.csv\")\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67f8203-111e-4a19-8cd4-24e96970308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   restaurantId                              restaurant  \\\n",
      "0             1                              Café Natsu   \n",
      "1             2    Mr. Holmes Bakehouse (Pacific Plaza)   \n",
      "2             3  Craftsmen Coffee (Clarke Quay Central)   \n",
      "3             4                            hay, gelato.   \n",
      "4             5                               Refuel II   \n",
      "\n",
      "                         genre price range  \n",
      "0                japanese|cafe    moderate  \n",
      "1         bakery|café|American    moderate  \n",
      "2          coffee|café|western    moderate  \n",
      "3                      dessert       cheap  \n",
      "4  chinese|indian|western|cafe    moderate  \n"
     ]
    }
   ],
   "source": [
    "# loading restaurant dataset\n",
    "restaurant = pd.read_csv(\"C:/Users/Garion/Desktop/Data for ML.csv\")\n",
    "print(restaurant.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79b15ac-9313-429b-810a-116b896122c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings: 84\n",
      "Number of unique restaurantId's: 69\n",
      "Number of unique users: 15\n",
      "Average ratings per user: 5.6\n",
      "Average ratings per restaurant: 1.22\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "n_ratings = len(ratings)\n",
    "n_restaurant = len(ratings['restaurantId'].unique())\n",
    "n_users = len(ratings['userId'].unique())\n",
    " \n",
    "print(f\"Number of ratings: {n_ratings}\")\n",
    "print(f\"Number of unique restaurantId's: {n_restaurant}\")\n",
    "print(f\"Number of unique users: {n_users}\")\n",
    "print(f\"Average ratings per user: {round(n_ratings/n_users, 2)}\")\n",
    "print(f\"Average ratings per restaurant: {round(n_ratings/n_restaurant, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c74e9b-c2c7-4632-a9ad-4c7d7119a47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  n_ratings\n",
      "0       1          5\n",
      "1       2          6\n",
      "2       3          6\n",
      "3       4          5\n",
      "4       5          5\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "user_freq = ratings[['userId', 'restaurantId']].groupby(\n",
    "    'userId').count().reset_index()\n",
    "user_freq.columns = ['userId', 'n_ratings']\n",
    "print(user_freq.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e8ce99-4a97-4d2f-86ee-4ebc961587f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Lowest and Highest rated restaurants:\n",
    "mean_rating = ratings.groupby('restaurantId')[['rating']].mean()\n",
    "# Lowest rated restaurants\n",
    "lowest_rated = mean_rating['rating'].idxmin()\n",
    "restaurant.loc[restaurant['restaurantId'] == lowest_rated]\n",
    "# Highest rated restaurants\n",
    "highest_rated = mean_rating['rating'].idxmax()\n",
    "restaurant.loc[restaurant['restaurantId'] == highest_rated]\n",
    "# show number of people who rated restaurants rated restaurant highest\n",
    "ratings[ratings['restaurantId']==highest_rated]\n",
    "# show number of people who rated restaurants rated restaurant lowest\n",
    "ratings[ratings['restaurantId']==lowest_rated]\n",
    " \n",
    "## the above restaurants has very low dataset. We will use bayesian average\n",
    "restaurant_stats = ratings.groupby('restaurantId')[['rating']].agg(['count', 'mean'])\n",
    "restaurant_stats.columns = restaurant_stats.columns.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0248c1-3faf-4cf7-8298-c3e3d4631577",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Now, we create user-item matrix using scipy csr matrix\n",
    "from scipy.sparse import csr_matrix\n",
    " \n",
    "def create_matrix(df):\n",
    "     \n",
    "    N = len(df['userId'].unique())\n",
    "    M = len(df['restaurantId'].unique())\n",
    "     \n",
    "    # Map Ids to indices\n",
    "    user_mapper = dict(zip(np.unique(df[\"userId\"]), list(range(N))))\n",
    "    restaurant_mapper = dict(zip(np.unique(df[\"restaurantId\"]), list(range(M))))\n",
    "     \n",
    "    # Map indices to IDs\n",
    "    user_inv_mapper = dict(zip(list(range(N)), np.unique(df[\"userId\"])))\n",
    "    restaurant_inv_mapper = dict(zip(list(range(M)), np.unique(df[\"restaurantId\"])))\n",
    "     \n",
    "    user_index = [user_mapper[i] for i in df['userId']]\n",
    "    restaurant_index = [restaurant_mapper[i] for i in df['restaurantId']]\n",
    " \n",
    "    X = csr_matrix((df[\"rating\"], (restaurant_index, user_index)), shape=(M, N))\n",
    "     \n",
    "    return X, user_mapper, restaurant_mapper, user_inv_mapper, restaurant_inv_mapper\n",
    "     \n",
    "X, user_mapper, restaurant_mapper, user_inv_mapper, restaurant_inv_mapper = create_matrix(ratings)\n",
    "#print(X)\n",
    "#print(\"\")\n",
    "#print(user_mapper)\n",
    "#print(user_inv_mapper)\n",
    "#print(restaurant_mapper)\n",
    "#print(restaurant_inv_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82a598a-a3c0-4850-9551-f7ae8f78df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since you watched Jimmy Monkey Cafe & Bar\n",
      "Sushi Kimura\n",
      "Itacho Sushi\n",
      "Muk-Bang Korean Restaurant\n",
      "Kei Kaisendon\n",
      "Brawn & Brains\n",
      "Seoul Garden\n",
      "Baci Baci Restaurant\n",
      "The Sushi Bar\n",
      "The Assembly Ground, Cineleisure (Somerset)\n",
      "Nickel\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find similar restaurants using KNN\n",
    "\"\"\"\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "def find_similar_restaurant(restaurant_id, X, k, metric='cosine', show_distance=False):\n",
    "     \n",
    "    neighbour_ids = []\n",
    "     \n",
    "    restaurant_ind = restaurant_mapper[restaurant_id]\n",
    "    restaurant_vec = X[restaurant_ind]\n",
    "    #print(restaurant_vec)\n",
    "    k += 1\n",
    "    kNN = NearestNeighbors(n_neighbors=k, algorithm=\"brute\", metric=metric)\n",
    "    kNN.fit(X)\n",
    "    restaurant_vec = restaurant_vec.reshape(1,-1)\n",
    "    neighbour = kNN.kneighbors(restaurant_vec, return_distance=show_distance)\n",
    "    for i in range(0,k):\n",
    "        n = neighbour.item(i)\n",
    "        neighbour_ids.append(restaurant_inv_mapper[n])\n",
    "    neighbour_ids.pop(0)\n",
    "    return neighbour_ids\n",
    " \n",
    " \n",
    "restaurant_titles = dict(zip(restaurant['restaurantId'], restaurant['restaurant']))\n",
    " \n",
    "restaurant_id = 7\n",
    " \n",
    "similar_ids = find_similar_restaurant(restaurant_id, X, k=10)\n",
    "restaurant_title = restaurant_titles[restaurant_id]\n",
    " \n",
    "print(f\"Since you watched {restaurant_title}\")\n",
    "for i in similar_ids:\n",
    "    print(restaurant_titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1824c915-c0cc-4157-b92e-0adc92358045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_restaurants_for_user(user_id, X, user_mapper, restaurant_mapper, restaurant_inv_mapper, k=10):\n",
    "    frames = []\n",
    "    #print(ratings.loc[ratings.userId == 1].empty)\n",
    "    for id in user_id:\n",
    "        #print(id)\n",
    "        if not ratings.loc[ratings.userId == id].empty:\n",
    "        #if ratings.userId.isin([id]).all():\n",
    "            frames.append(ratings[ratings['userId'] == id])\n",
    "            df1 = pd.concat(frames)\n",
    "        else:\n",
    "            print(f\"one of the User with ID {id} does not exist.\")\n",
    "            return\n",
    "    if df1.empty:\n",
    "        print(f\"User with ID {user_id} does not exist.\")\n",
    "        return\n",
    "    #print(frames)\n",
    "    \n",
    "    #print(\"\")\n",
    "    print(df1)\n",
    "    print(\"\")\n",
    "    restaurant_id = df1[df1['rating'] == max(df1['rating'])]['restaurantId'].iloc[0]\n",
    " \n",
    "    restaurant_titles = dict(zip(restaurant['restaurantId'], restaurant['restaurant']))\n",
    " \n",
    "    similar_ids = find_similar_restaurant(restaurant_id, X, k)\n",
    "    restaurant_title = restaurant_titles.get(restaurant_id, \"restaurant not found\")\n",
    " \n",
    "    if restaurant_title == \"restaurant not found\":\n",
    "        print(f\"restaurant with ID {restaurant_id} not found.\")\n",
    "        return\n",
    " \n",
    "    print(f\"Since you ate at {restaurant_title}, you might also like:\")\n",
    "    for i in similar_ids:\n",
    "        print(restaurant_titles.get(i, \"restaurant not found\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0eecd6e-45ed-45e3-94ae-3a17c44c1561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    userId  restaurantId  rating\n",
      "0        1           111     3.0\n",
      "1        1           107     4.5\n",
      "2        1            65     4.6\n",
      "3        1            35     3.6\n",
      "4        1            86     4.0\n",
      "5        2           106     3.3\n",
      "6        2            91     4.2\n",
      "7        2             8     3.8\n",
      "8        2            83     4.3\n",
      "9        2            65     4.0\n",
      "10       2           130     4.9\n",
      "\n",
      "Since you ate at Nangfa Thai Kitchen, you might also like:\n",
      "Harry's\n",
      "OLLA Specialty Coffee (Clementi)\n",
      "Tipo Pasta Bar\n",
      "Sawadee Thai Cuisine\n",
      "Yamagawa Japanese Restaurant, Beach Road\n",
      "Kra pow Thai Restaurant\n",
      "Coexist Coffee Co.\n",
      "Tora Tora Tora Japanese Restaurant Singapore\n",
      "Hansik Korean Restaurant\n",
      "OTOKO Japanese Restaurant\n"
     ]
    }
   ],
   "source": [
    "user_id = {1,2}  # Replace with the desired user ID\n",
    "recommend_restaurants_for_user(user_id, X, user_mapper, restaurant_mapper, restaurant_inv_mapper, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414bb2e8-eaf6-4815-87f8-84eb826be4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one of the User with ID 2300 does not exist.\n"
     ]
    }
   ],
   "source": [
    "user_id = {2300}  # Replace with the desired user ID\n",
    "recommend_restaurants_for_user(user_id, X, user_mapper, restaurant_mapper, restaurant_inv_mapper, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dec46-e9be-4294-8fac-c657671b9372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#loading rating dataset\n",
    "ratings = pd.read_csv(\"C:/Users/Garion/Desktop/user preference.csv\")\n",
    "print(ratings.head())\n",
    "# loading restaurant dataset\n",
    "restaurant = pd.read_csv(\"C:/Users/Garion/Desktop/Data for ML.csv\")\n",
    "print(restaurant.head())\n",
    "n_ratings = len(ratings)\n",
    "n_restaurant = len(ratings['restaurantId'].unique())\n",
    "n_users = len(ratings['userId'].unique())\n",
    " \n",
    "print(f\"Number of ratings: {n_ratings}\")\n",
    "print(f\"Number of unique restaurantId's: {n_restaurant}\")\n",
    "print(f\"Number of unique users: {n_users}\")\n",
    "print(f\"Average ratings per user: {round(n_ratings/n_users, 2)}\")\n",
    "print(f\"Average ratings per restaurant: {round(n_ratings/n_restaurant, 2)}\") \n",
    "user_freq = ratings[['userId', 'restaurantId']].groupby(\n",
    "    'userId').count().reset_index()\n",
    "user_freq.columns = ['userId', 'n_ratings']\n",
    "print(user_freq.head())\n",
    "# Find Lowest and Highest rated restaurants:\n",
    "mean_rating = ratings.groupby('restaurantId')[['rating']].mean()\n",
    "# Lowest rated restaurants\n",
    "lowest_rated = mean_rating['rating'].idxmin()\n",
    "restaurant.loc[restaurant['restaurantId'] == lowest_rated]\n",
    "# Highest rated restaurants\n",
    "highest_rated = mean_rating['rating'].idxmax()\n",
    "restaurant.loc[restaurant['restaurantId'] == highest_rated]\n",
    "# show number of people who rated restaurants rated restaurant highest\n",
    "ratings[ratings['restaurantId']==highest_rated]\n",
    "# show number of people who rated restaurants rated restaurant lowest\n",
    "ratings[ratings['restaurantId']==lowest_rated]\n",
    " \n",
    "## the above restaurants has very low dataset. We will use bayesian average\n",
    "restaurant_stats = ratings.groupby('restaurantId')[['rating']].agg(['count', 'mean'])\n",
    "restaurant_stats.columns = restaurant_stats.columns.droplevel()  \n",
    "# Now, we create user-item matrix using scipy csr matrix\n",
    "from scipy.sparse import csr_matrix\n",
    " \n",
    "def create_matrix(df):\n",
    "     \n",
    "    N = len(df['userId'].unique())\n",
    "    M = len(df['restaurantId'].unique())\n",
    "     \n",
    "    # Map Ids to indices\n",
    "    user_mapper = dict(zip(np.unique(df[\"userId\"]), list(range(N))))\n",
    "    restaurant_mapper = dict(zip(np.unique(df[\"restaurantId\"]), list(range(M))))\n",
    "     \n",
    "    # Map indices to IDs\n",
    "    user_inv_mapper = dict(zip(list(range(N)), np.unique(df[\"userId\"])))\n",
    "    restaurant_inv_mapper = dict(zip(list(range(M)), np.unique(df[\"restaurantId\"])))\n",
    "     \n",
    "    user_index = [user_mapper[i] for i in df['userId']]\n",
    "    restaurant_index = [restaurant_mapper[i] for i in df['restaurantId']]\n",
    " \n",
    "    X = csr_matrix((df[\"rating\"], (restaurant_index, user_index)), shape=(M, N))\n",
    "     \n",
    "    return X, user_mapper, restaurant_mapper, user_inv_mapper, restaurant_inv_mapper\n",
    "     \n",
    "X, user_mapper, restaurant_mapper, user_inv_mapper, restaurant_inv_mapper = create_matrix(ratings)\n",
    "print(X)\n",
    "print(\"\")\n",
    "print(user_mapper)\n",
    "print( restaurant_mapper)\n",
    "print(user_inv_mapper)\n",
    "print(restaurant_inv_mapper)\n",
    "\"\"\"\n",
    "Find similar restaurants using KNN\n",
    "\"\"\"\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "def find_similar_restaurant(restaurant_id, X, k, metric='cosine', show_distance=False):\n",
    "     \n",
    "    neighbour_ids = []\n",
    "    print(restaurant_id)\n",
    "    print(X)\n",
    "    restaurant_ind = restaurant_mapper[restaurant_id]\n",
    "    print(\"rest id\")\n",
    "    print(restaurant_ind)\n",
    "    restaurant_vec = X[restaurant_ind]\n",
    "    print(restaurant_vec)\n",
    "    k+=1\n",
    "    kNN = NearestNeighbors(n_neighbors=k, algorithm=\"brute\", metric=metric)\n",
    "    kNN.fit(X)\n",
    "    restaurant_vec = restaurant_vec.reshape(1,-1)\n",
    "    neighbour = kNN.kneighbors(restaurant_vec, return_distance=show_distance)\n",
    "    for i in range(0,k):\n",
    "        n = neighbour.item(i)\n",
    "        neighbour_ids.append(restaurant_inv_mapper[n])\n",
    "    neighbour_ids.pop(0)\n",
    "    return neighbour_ids, kNN\n",
    " \n",
    " \n",
    "restaurant_titles = dict(zip(restaurant['restaurantId'], restaurant['restaurant']))\n",
    " \n",
    "restaurant_id = 8\n",
    " \n",
    "similar_ids, kNN_model= find_similar_restaurant(restaurant_id, X, k=10)\n",
    "restaurant_title = restaurant_titles[restaurant_id]\n",
    " \n",
    "print(f\"Since you watched {restaurant_title}\")\n",
    "for i in similar_ids:\n",
    "    print(restaurant_titles[i])\n",
    "\n",
    "def recommend_restaurants_for_user(user_id, X, user_mapper, restaurant_mapper, restaurant_inv_mapper, k=10):\n",
    "    frames = []\n",
    "    #print(ratings.loc[ratings.userId == 1].empty)\n",
    "    for id in user_id:\n",
    "        print(id)\n",
    "        if not ratings.loc[ratings.userId == id].empty:\n",
    "        #if ratings.userId.isin([id]).all():\n",
    "            frames.append(ratings[ratings['userId'] == id])\n",
    "            df1 = pd.concat(frames)\n",
    "        else:\n",
    "            print(f\"one of the User with ID {id} does not exist.\")\n",
    "            return\n",
    "    if df1.empty:\n",
    "        print(f\"User with ID {user_id} does not exist.\")\n",
    "        return\n",
    "    print(frames)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(df1)\n",
    "    restaurant_id = df1[df1['rating'] == max(df1['rating'])]['restaurantId'].iloc[0]\n",
    " \n",
    "    restaurant_titles = dict(zip(restaurant['restaurantId'], restaurant['restaurant']))\n",
    " \n",
    "    similar_ids, Knn_model = find_similar_restaurant(restaurant_id, X, k)\n",
    "    restaurant_title = restaurant_titles.get(restaurant_id, \"restaurant not found\")\n",
    " \n",
    "    if restaurant_title == \"restaurant not found\":\n",
    "        print(f\"restaurant with ID {restaurant_id} not found.\")\n",
    "        return\n",
    " \n",
    "    #print(f\"Since you ate at {restaurant_title}, you might also like:\")\n",
    "    #for i in similar_ids:\n",
    "    #    print(restaurant_titles.get(i, \"restaurant not found\"))\n",
    "    return similar_ids, restaurant_titles, restaurant_title, Knn_model\n",
    "user_id = [1,2]  # Replace with the desired user ID\n",
    "print(similar_ids)\n",
    "print(restaurant_titles)\n",
    "similar_ids, restaurant_titles, restaurant_title, Knn_model = recommend_restaurants_for_user(user_id, X, user_mapper, restaurant_mapper, restaurant_inv_mapper, k=10)\n",
    "\n",
    "print(f\"Since you ate at {restaurant_title}, you might also like:\")\n",
    "for i in similar_ids:\n",
    "    print(restaurant_titles.get(i, \"restaurant not found\"))\n",
    "    \n",
    "#import joblib\n",
    "#import os\n",
    "\n",
    "# Check if the directory exists, if not, create it.\n",
    "#if not os.path.exists('models'):\n",
    "#    os.makedirs('models')\n",
    "\n",
    "# Save the model\n",
    "similar_ids, kNN_model = find_similar_restaurant(7, X, k=10)\n",
    "print(kNN_model)\n",
    "#joblib.dump(kNN_model, 'models/knn_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3563e0-baae-4cd2-ae82-d0237f8c6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = {2300}  # Replace with the desired user ID\n",
    "recommend_restaurants_for_user(user_id, X, user_mapper, restaurant_mapper, restaurant_inv_mapper, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65145a41-f6f9-44d2-873f-9586a605e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = joblib.load('models/knn_model.joblib')\n",
    "print(knn_model)\n",
    "# Define a new data point\n",
    "X_new = [[1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15]]\n",
    "\n",
    "k = 5\n",
    "\n",
    "M = len(ratings['restaurantId'].unique())\n",
    "\n",
    "restaurant_inv_mapper = dict(zip(list(range(M)), np.unique(ratings[\"restaurantId\"])))\n",
    "#X, user_mapper, restaurant_mapper, user_inv_mapper, restaurant_inv_mapper = create_matrix(ratings)\n",
    "#indices = knn_model.kneighbors(X_new, return_distance=False)[0]\n",
    "\n",
    "neighbour_ids = []\n",
    "\n",
    "neighbour = knn_model.kneighbors(X_new, return_distance=False)\n",
    "for i in range(0,5):\n",
    "    n = neighbour.item(i)\n",
    "    neighbour_ids.append(restaurant_inv_mapper[n])\n",
    "\n",
    "\n",
    "restaurant_titles = dict(zip(restaurant['restaurantId'], restaurant['restaurant']))\n",
    "print(restaurant_titles)\n",
    "print(\"\")\n",
    "print(neighbour)\n",
    "# Get the labels of the k nearest neighbors\n",
    "#labels = knn_model.labels_[indices]\n",
    "\n",
    "# Make a prediction using the labels of the k nearest neighbors\n",
    "for i in neighbour_ids:\n",
    "    print(restaurant_titles.get(i, \"restaurant not found\"))\n",
    "#prediction = max(set(labels), key=labels.count)\n",
    "\n",
    "#print(predition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec556de-be8d-4820-b92b-e6792ddbab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Dense, Flatten, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#loading rating dataset\n",
    "ratings = pd.read_csv(\"C:/Users/Garion/Desktop/user preference.csv\")\n",
    "\n",
    "# loading restaurant dataset\n",
    "restaurant = pd.read_csv(\"C:/Users/Garion/Desktop/Data for ML.csv\")\n",
    "\n",
    "# Create user and restaurant embeddings\n",
    "user_embedding = Embedding(input_dim=len(ratings['userId'].unique()), output_dim=50)\n",
    "restaurant_embedding = Embedding(input_dim=len(ratings['restaurantId'].unique()), output_dim=50)\n",
    "\n",
    "# Flatten the embeddings\n",
    "user_embedding = Flatten()(user_embedding(tf.convert_to_tensor(ratings['userId'].unique())))\n",
    "restaurant_embedding = Flatten()(restaurant_embedding(tf.convert_to_tensor(ratings['restaurantId'].unique())))\n",
    "\n",
    "# Calculate the dot product between the user and restaurant embeddings\n",
    "dot_product = Dot(axes=1)([user_embedding, restaurant_embedding])\n",
    "\n",
    "# Add a bias term\n",
    "dot_product = Dot(axes=1, normalize=False)([user_embedding, restaurant_embedding])\n",
    "\n",
    "# Add a dense layer to output the ratings\n",
    "output = Dense(1, activation='linear')(dot_product)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[user_embedding, restaurant_embedding], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
    "\n",
    "# Save the model\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "model.save('models/restaurant_rating_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db3282-45f4-4529-a5ef-6a0abc18aa66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#loading rating dataset\n",
    "ratings = pd.read_csv(\"C:/Users/Garion/Desktop/user preference.csv\")\n",
    "\n",
    "# loading restaurant dataset\n",
    "restaurant = pd.read_csv(\"C:/Users/Garion/Desktop/Data for ML.csv\")\n",
    "\n",
    "n_ratings = len(ratings)\n",
    "n_restaurant = len(ratings['restaurantId'].unique())\n",
    "n_users = len(ratings['userId'].unique())\n",
    "\n",
    "print(f\"Number of ratings: {n_ratings}\")\n",
    "print(f\"Number of unique restaurantId's: {n_restaurant}\")\n",
    "print(f\"Number of unique users: {n_users}\")\n",
    "print(f\"Average ratings per user: {round(n_ratings/n_users, 2)}\")\n",
    "print(f\"Average ratings per restaurant: {round(n_ratings/n_restaurant, 2)}\") \n",
    "\n",
    "user_freq = ratings[['userId', 'restaurantId']].groupby(\n",
    "    'userId').count().reset_index()\n",
    "user_freq.columns = ['userId', 'n_ratings']\n",
    "\n",
    "# Find Lowest and Highest rated restaurants:\n",
    "mean_rating = ratings.groupby('restaurantId')[['rating']].mean()\n",
    "\n",
    "# Now, we create user-item matrix using scipy csr matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def create_matrix(df):\n",
    "     \n",
    "    N = len(df['userId'].unique())\n",
    "    M = len(df['restaurantId'].unique())\n",
    "     \n",
    "    # Map Ids to indices\n",
    "    user_mapper = dict(zip(np.unique(df[\"userId\"]), list(range(N))))\n",
    "    restaurant_mapper = dict(zip(np.unique(df[\"restaurantId\"]), list(range(M))))\n",
    "     \n",
    "    # Map indices to IDs\n",
    "    user_inv_mapper = dict(zip(list(range(N)), np.unique(df[\"userId\"])))\n",
    "    restaurant_inv_mapper = dict(zip(list(range(M)), np.unique(df[\"restaurantId\"])))\n",
    "     \n",
    "    user_index = [user_mapper[i] for i in df['userId']]\n",
    "    restaurant_index = [restaurant_mapper[i] for i in df['restaurantId']]\n",
    " \n",
    "    X = csr_matrix((df[\"rating\"], (restaurant_index, user_index)), shape=(M, N))\n",
    "     \n",
    "    return X, user_mapper, restaurant_mapper, user_inv_mapper, restaurant_inv_mapper\n",
    "     \n",
    "X, user_mapper, restaurant_mapper, user_inv_mapper, restaurant_inv_mapper = create_matrix(ratings)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def find_similar_restaurant(restaurant_id, X, k, metric='cosine', show_distance=False):\n",
    "     \n",
    "    restaurant_ind = restaurant_mapper[restaurant_id]\n",
    "    \n",
    "    restaurant_vec = X[restaurant_ind]\n",
    "    k += 1\n",
    "    \n",
    "    # Initialize the KNeighborsClassifier with the 'cosine' metric and 10 neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
    "    \n",
    "    # Fit the classifier to the data\n",
    "    knn.fit(X, np.arange(X.shape[0]))\n",
    "    \n",
    "    # Predict the similarity scores for the target restaurant vector\n",
    "    similarity_scores = knn.predict_proba(restaurant_vec.reshape(1, -1))\n",
    "    \n",
    "    # Get the indices of the top 10 similar restaurants\n",
    "    similar_indices = np.argsort(similarity_scores.flatten())[::-1][1:11]\n",
    "    \n",
    "    # Map the indices to the corresponding restaurant IDs\n",
    "    similar_ids = [restaurant_inv_mapper[i] for i in similar_indices]\n",
    "    \n",
    "    joblib.dump(knn, 'models/knn_model.joblib')\n",
    "    \n",
    "    return similar_ids, knn\n",
    "\n",
    "restaurant_titles = dict(zip(restaurant['restaurantId'], restaurant['restaurant']))\n",
    "\n",
    "restaurant_id = 7\n",
    "\n",
    "similar_ids, knn = find_similar_restaurant(restaurant_id, X, k=10)\n",
    "\n",
    "print(f\"Since you watched {restaurant_titles[restaurant_id]}, you might also like:\")\n",
    "for i in similar_ids:\n",
    "    print(restaurant_titles.get(i, \"restaurant not found\"))\n",
    "\n",
    "\n",
    "\n",
    "def recommend_restaurants_for_user(user_id, X, user_mapper, restaurant_mapper, restaurant_inv_mapper, k=10):\n",
    "    frames = []\n",
    "    #print(ratings.loc[ratings.userId == 1].empty)\n",
    "    for id in user_id:\n",
    "        print(id)\n",
    "        if not ratings.loc[ratings.userId == id].empty:\n",
    "        #if ratings.userId.isin([id]).all():\n",
    "            frames.append(ratings[ratings['userId'] == id])\n",
    "            df1 = pd.concat(frames)\n",
    "        else:\n",
    "            print(f\"one of the User with ID {id} does not exist.\")\n",
    "            return\n",
    "    if df1.empty:\n",
    "        print(f\"User with ID {user_id} does not exist.\")\n",
    "        return\n",
    "    print(frames)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(df1)\n",
    "    restaurant_id = df1[df1['rating'] == max(df1['rating'])]['restaurantId'].iloc[0]\n",
    " \n",
    "    restaurant_titles = dict(zip(restaurant['restaurantId'], restaurant['restaurant']))\n",
    " \n",
    "    similar_ids, KNN = find_similar_restaurant(restaurant_id, X, k)\n",
    "    restaurant_title = restaurant_titles.get(restaurant_id, \"restaurant not found\")\n",
    " \n",
    "    if restaurant_title == \"restaurant not found\":\n",
    "        print(f\"restaurant with ID {restaurant_id} not found.\")\n",
    "        return\n",
    " \n",
    "    #print(f\"Since you ate at {restaurant_title}, you might also like:\")\n",
    "    #for i in similar_ids:\n",
    "    #    print(restaurant_titles.get(i, \"restaurant not found\"))\n",
    "    return similar_ids, restaurant_titles, restaurant_title, KNN\n",
    "\n",
    "user_id = [1,2]  # Replace with the desired user ID\n",
    "print(similar_ids)\n",
    "print(restaurant_titles)\n",
    "similar_ids, restaurant_titles, restaurant_title, KNN = recommend_restaurants_for_user(user_id, X, user_mapper, restaurant_mapper, restaurant_inv_mapper, k=10)\n",
    "\n",
    "print(f\"Since you ate at {restaurant_title}, you might also like:\")\n",
    "for i in similar_ids:\n",
    "    print(restaurant_titles.get(i, \"restaurant not found\"))\n",
    "\n",
    "print(\"\")\n",
    "print(KNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695f817-08c3-4870-8662-85f7d0755272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b818b87-ae1b-427b-9edc-3df042e7e9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
